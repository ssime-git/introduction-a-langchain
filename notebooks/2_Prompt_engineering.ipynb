{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering avec Langchain\n",
    "\n",
    "Les grands modèles linguistiques (LLM) peuvent accomplir toutes les tâches de classification, de question \\ réponse (autrefois réalisées par des modèles de ML ou encore de DL utilisant l'architecture des transformers) et bien plus encore. \n",
    "\n",
    "Ces modèles ont été formés à partir d'un concept simple : vous saisissez une séquence de texte (`Prompt`) et le modèle produit une séquence de texte (ou un image). La seule variable ici est le texte d'entrée - l'invite ou le prompt.\n",
    "\n",
    "Dans cette nouvelle ère des LLM, les prompts sont rois. Les mauvais messages-guides produisent de mauvais résultats, et les bons messages-guides sont déraisonnablement puissants. Construire de bons messages-guides est une compétence cruciale pour ceux qui construisent avec des LLM.\n",
    "\n",
    "La bibliothèque LangChain reconnaît la puissance des invites et a construit un ensemble complet d'objets pour elles.\n",
    "\n",
    "\n",
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutes les invites n'utilisent pas ces composantes, mais une bonne invite en utilise souvent deux ou plus. Définissons-les plus précisément.\n",
    "\n",
    "- Les instructions indiquent au modèle ce qu'il doit faire, comment utiliser les informations externes si elles sont fournies, ce qu'il doit faire avec la requête et comment construire la sortie.\n",
    "\n",
    "- Les informations externes ou le(s) contexte(s) constituent une source supplémentaire de connaissances pour le modèle. Elles peuvent être insérées manuellement dans l'invite, récupérées par l'intermédiaire d'une base de données vectorielle (augmentation de la récupération) ou introduites par d'autres moyens (API, calculs, etc.).\n",
    "\n",
    "- L'entrée ou la requête de l'utilisateur est généralement (mais pas toujours) une requête introduite dans le système par un utilisateur humain (le prompteur).\n",
    "\n",
    "- L'indicateur de sortie marque le début du texte à générer. Si l'on génère du code Python, on peut utiliser import pour indiquer au modèle qu'il doit commencer à écrire du code Python (car la plupart des scripts Python commencent par import).\n",
    "\n",
    "Chaque composant est généralement placé dans l'invite dans l'ordre suivant :\n",
    "\n",
    "1. Instructions, \n",
    "2. Les informations externes (le cas échéant), \n",
    "3. L'entrée du prompteur,\n",
    "4. L'indicateur de sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from langchain.chat_models import AzureChatOpenAI OLD\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the question based on the context below. If the\n",
      "question cannot be answered using the information provided answer\n",
      "with \"I don't know\".\n",
      "\n",
      "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
      "Their superior performance over smaller models has made them incredibly\n",
      "useful for developers building NLP enabled applications. These models\n",
      "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
      "using the `openai` library, and via Cohere using the `cohere` library.\n",
      "\n",
      "Question: Which libraries and model providers offer LLMs?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# just to print the template\n",
    "print(\n",
    "    prompt_template.format(\n",
    "        query=\"Which libraries and model providers offer LLMs?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sait déjà que pour créer une chaîne avec Langchain il faut combiner un modèle de langage avec une invite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Library | LLM Provider |\n",
      "|---------|--------------|\n",
      "| `transformers` | Hugging Face |\n",
      "| `openai` | OpenAI |\n",
      "| `cohere` | Cohere |\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "# Model definition\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-09-01-preview\",\n",
    "    azure_endpoint=os.getenv('AZURE_API_ENDPOINT'),\n",
    "    api_key=os.getenv('AZURE_OPENAI_KEY'),\n",
    "    azure_deployment=os.getenv('OPENAI_DEPLOYMENT_NAME'),\n",
    "    model_name=os.getenv('OPENAI_MODEL_NAME'),\n",
    "    model_version=os.getenv('OPENAI_API_VERSION'),\n",
    "    temperature=.7\n",
    ")\n",
    "\n",
    "\n",
    "# Create the LLMChain instance\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt_template ,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Provide a question\n",
    "question = \"\"\"\n",
    "List libraries and model providers offer LLMs in a Markdown table.\n",
    "\"\"\"\n",
    "\n",
    "# Get the answer\n",
    "answer = llm_chain.invoke(input=question)\n",
    "print(answer['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Prompt\n",
    "\n",
    "Le succès des LLM provient de leur grande taille et de leur capacité à stocker la \"connaissance\" dans le paramètre du modèle, qui est appris au cours de l'apprentissage du modèle. Cependant, il y a plusieurs façons de transmettre la connaissance à un LLM. \n",
    "\n",
    "Les deux méthodes principales sont les suivantes :\n",
    "\n",
    "- Connaissance paramétrique - la connaissance mentionnée ci-dessus est tout ce qui a été appris par le modèle pendant la formation et qui est stocké dans les poids (ou paramètres) du modèle.\n",
    "\n",
    "- Connaissance de la source - toute connaissance fournie au modèle au moment de l'inférence par l'intermédiaire de l'invite d'entrée.\n",
    "\n",
    "Le modèle `FewShotPromptTemplate` de Langchain s'occupe de l'entrée de la connaissance source. L'idée est de \"former\" le modèle sur quelques exemples - nous appelons cela l'apprentissage en quelques coups - et ces exemples sont donnés au modèle dans l'invite.\n",
    "\n",
    "L'apprentissage ponctuel est idéal lorsque notre modèle a besoin d'aide pour comprendre ce que nous lui demandons de faire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples, # to pass example\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# print the complete prompt :\n",
    "query = \"What is the meaning of life?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: 42\n",
      "\n",
      "\n",
      "User: How do birds fly?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "# to manage the context window = input tokens + output tokens\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "\n",
    "# to change the number of examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the meaning of life?\",\n",
    "        \"answer\": \"42\"\n",
    "    }, {\n",
    "        \"query\": \"What is the weather like today?\",\n",
    "        \"answer\": \"Cloudy with a chance of memes.\"\n",
    "    }, {\n",
    "        \"query\": \"What is your favorite movie?\",\n",
    "        \"answer\": \"Terminator\"\n",
    "    }, {\n",
    "        \"query\": \"Who is your best friend?\",\n",
    "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
    "    }, {\n",
    "        \"query\": \"What should I do today?\",\n",
    "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# to select only some examples\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=50  # this sets the max length that examples should be\n",
    ")\n",
    "\n",
    "\n",
    "# now create the few shot prompt template\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,  # use example_selector instead of examples\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")\n",
    "\n",
    "# display the prompt\n",
    "print(dynamic_prompt_template.format(query=\"How do birds fly?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous répondez à une question plus longue, vous aurez moins d'exemples :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: If I am in America, and I want to call someone in another country, I'm\n",
      "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
      "what is the best way to do that?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"If I am in America, and I want to call someone in another country, I'm\n",
    "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
    "what is the best way to do that?\"\"\"\n",
    "\n",
    "print(dynamic_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, you could always try shouting really loudly across the ocean, but I'm not sure how effective that would be. Alternatively, you could use a phone or internet service provider that offers international calling options. Just a thought.\n"
     ]
    }
   ],
   "source": [
    "# Create the LLMChain instance\n",
    "llm_chain = LLMChain(\n",
    "    prompt=dynamic_prompt_template ,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Get the answer\n",
    "answer = llm_chain.run(query=query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifier le format de sortie\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
      "\n",
      "text: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# define the field\n",
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas_list = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]\n",
    "\n",
    "# Define the structure\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas_list)\n",
    "\n",
    "# Get the formated instruction\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review,\n",
    "                                  format_instructions=format_instructions)\n",
    "\n",
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": true,\n",
      "\t\"delivery_days\": \"2\",\n",
      "\t\"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': True,\n",
       " 'delivery_days': '2',\n",
       " 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "output_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
