{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction LangChain\n",
    "\n",
    "LangChain est un framework populaire qui permet aux utilisateurs de construire rapidement des applications et des pipelines autour de grands modèles de langage. Il peut être utilisé pour les chatbots, la réponse générative aux questions, le résumé, et bien plus encore.\n",
    "\n",
    "L'idée centrale de la bibliothèque est que nous pouvons \"enchaîner\" différents composants pour créer des cas d'utilisation plus avancés autour des LLM. Les chaînes peuvent être constituées de plusieurs composants provenant de plusieurs modules :\n",
    "\n",
    "- **Modèles d'invite** : Les modèles d'invites sont, en fait, des modèles pour différents types d'invites. Comme les modèles de style \"chatbot\", les réponses aux questions ELI5, etc.\n",
    "\n",
    "- **LLM** : Grands modèles de langage comme GPT-3, BLOOM, etc.\n",
    "\n",
    "- **Agents** : Les agents utilisent les LLM pour décider des actions à entreprendre, des outils tels que la recherche sur le web ou des calculatrices peuvent être utilisés, le tout étant intégré dans une boucle logique d'opérations.\n",
    "\n",
    "- **Mémoire** : Mémoire à court terme, mémoire à long terme.\n",
    "\n",
    "Mais commençons simplement avec les élements de base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the environments var\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# get hugging face API key\n",
    "HUGGINGFACEHUB_API_TOKEN = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to authenticate in notebook (from jupyter or colab only)\n",
    "#from huggingface_hub import notebook_login\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using simple base model FLAN-T5 (Google)\n",
    "\n",
    "Une chaîne simple est la réunion d'un LLM et d'un template de prompt (Prompt template). Nous allons commencer par montrer qu'il est possible de créer une chaîne simple avec un \"petit\" LLM comme `FLAN-T5` de Google.\n",
    "\n",
    "On va se servir de la plateforme `HuggingFace` pour charger le modèle `FLAN-T5` de Google.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Quelle est la capital de la  France?', 'text': 'paris'}\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "\n",
    "# Load the model from HuggingFace\n",
    "flan_t5 = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\",\n",
    "    model_kwargs={\"temperature\": 1e-10},\n",
    ")\n",
    "\n",
    "# Build prompt template for simple question-answering\n",
    "template = \"\"\"\n",
    "Question: {question}\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "# Create the LangChain prompt template\n",
    "prompt = PromptTemplate(template=template, \n",
    "                        input_variables=[\"question\"])\n",
    "\n",
    "# Create the LLMChain instance (A simple chain)\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt, # prompt template\n",
    "    llm=flan_t5 # LLM\n",
    ")\n",
    "\n",
    "# Define the question\n",
    "question = \"Quelle est la capital de la  France?\"\n",
    "\n",
    "# Run the chain and print the result\n",
    "print(llm_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de poser plusieurs questions à la fois d'un seul coup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions': 'Quelle est la Capital de la France?\\nCombien font 1 + 1?\\nWho was the 12th person on the moon?', 'text': 'Quelle est la Capitale de la France? Combien font 1 + 1? Who was'}\n"
     ]
    }
   ],
   "source": [
    "# basic prompt\n",
    "multi_template = \"\"\"\n",
    "Répondre à chacune des questions ci-dessous.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "\n",
    "# prompt template\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "# Create the LLMChain instance\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=flan_t5\n",
    ")\n",
    "\n",
    "# list all the questions\n",
    "qs_str = (\n",
    "    \"Quelle est la Capital de la France?\\n\" +\n",
    "    \"Combien font 1 + 1?\\n\" +\n",
    "    \"Who was the 12th person on the moon?\"\n",
    ")\n",
    "\n",
    "print(llm_chain.invoke(qs_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir les limitations du modèle FLAN-T5 qui peut-être utilisé pour des questions simples mais qui ne peut pas répondre à des questions plus complexes.\n",
    "\n",
    "C'est la raison pour laquelle nous allons utiliser un modèle plus puissant comme GPT-3 de OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation de Open AI\n",
    "\n",
    "Commençons par montrer ce que cela donne lorsque l'on utilise les fonctions natives de OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am an AI assistant designed to help you find information. What can I help you with today?\n"
     ]
    }
   ],
   "source": [
    "#Note: The openai-python library support for Azure OpenAI is in preview.\n",
    "      #Note: This code sample requires OpenAI Python library version 1.0.0 or higher.\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv('AZURE_API_ENDPOINT'), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version=os.getenv('OPENAI_API_VERSION')\n",
    ")\n",
    "\n",
    "# define the message\n",
    "message_text = [{\n",
    "    \"role\":\"system\",\n",
    "    \"content\":\"You are an AI assistant that helps people find information.\"}]\n",
    "\n",
    "# get the completion\n",
    "completion = llm.chat.completions.create(\n",
    "  model=\"gpt35turbo\", # model = \"deployment_name\"\n",
    "  messages = message_text,\n",
    "  temperature=0.0,\n",
    "  max_tokens=800,\n",
    "  top_p=0.95,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=None\n",
    ")\n",
    "\n",
    "# print the result\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, many other Azure AI services support customer managed keys, including Azure Cognitive Services, Azure Machine Learning, and Azure Databricks. Customer managed keys provide enhanced security and control over data encryption and are a recommended practice for many Azure services.\n"
     ]
    }
   ],
   "source": [
    "# give more elements\n",
    "response = llm.chat.completions.create(\n",
    "    model=\"gpt35turbo\", # model = \"deployment_name\".\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Does Azure OpenAI support customer managed keys?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Yes, customer managed keys are supported by Azure OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Do other Azure AI services support this too?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant avec LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\Documents\\projet\\data-atelier-intro-langchain\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La Capitale de la France est Paris.\n",
      "1 + 1 égalent 2.\n",
      "Le 12ème homme sur la lune était Eugene Cernan.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "#from langchain.chat_models import AzureChatOpenAI # OLD\n",
    "from langchain_openai import AzureChatOpenAI # Chat model abstraction class\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-09-01-preview\",\n",
    "    azure_endpoint=os.getenv('AZURE_API_ENDPOINT'),\n",
    "    api_key=os.getenv('AZURE_OPENAI_KEY'),\n",
    "    azure_deployment=os.getenv('OPENAI_DEPLOYMENT_NAME'),\n",
    "    model_name=os.getenv('OPENAI_MODEL_NAME'),\n",
    "    model_version=os.getenv('OPENAI_API_VERSION')  # Add the desired version identifier\n",
    ")\n",
    "\n",
    "\n",
    "# Define the prompt template\n",
    "multi_template = \"\"\"\n",
    "Répondre à chacune des questions ci-dessous.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "# Create the LLMChain instance\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt, # prompt template\n",
    "    llm=llm # Azure openai LLM\n",
    ")\n",
    "\n",
    "# List all the questions\n",
    "qs_str = (\n",
    "    \"Quelle est la Capitale de la France?\\n\" +\n",
    "    \"Combien font 1 + 1?\\n\" +\n",
    "    \"Who was the 12th person on the moon?\"\n",
    ")\n",
    "\n",
    "print(llm_chain.run(qs_str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir la puissance d'un modèle comme GPT-3 de OpenAI.\n",
    "\n",
    "## Récap:\n",
    "\n",
    "Les chaînes constituent le cœur de LangChain. Il s'agit simplement d'une chaîne de composants, exécutés dans un ordre particulier.\n",
    "\n",
    "La plus simple de ces chaînes est la chaîne `LLMChain`. Elle fonctionne en prenant l'entrée d'un utilisateur, en la passant au premier élément de la chaîne - un PromptTemplate - pour formater l'entrée en une invite particulière. L'invite formatée est ensuite transmise à l'élément suivant (et final) de la chaîne, un LLM.\n",
    "\n",
    "Les chaînes sont divisées en trois types : Les chaînes utilitaires, les chaînes génériques et les chaînes de combinaison de documents. Dans cette édition, nous nous concentrerons sur les deux premières, la troisième étant trop spécifique (elle sera abordée en temps voulu).\n",
    "\n",
    "- Chaînes utilitaires : chaînes généralement utilisées pour extraire une réponse spécifique d'un formulaire de demande de renseignements, dans un but très précis, et prêtes à l'emploi.\n",
    "- Chaînes génériques : chaînes utilisées comme blocs de construction pour d'autres chaînes, mais qui ne peuvent pas être utilisées seules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "What is the square root of 16?\u001b[32;1m\u001b[1;3m```text\n",
      "16**(0.5)\n",
      "```\n",
      "...numexpr.evaluate(\"16**(0.5)\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m4.0\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 231 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer: 4.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utility chains example\n",
    "from langchain.chains import LLMChain, LLMMathChain, TransformChain, SequentialChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def count_tokens(chain, query):\n",
    "    \"\"\"\n",
    "    Count the number of tokens used by the chain.\n",
    "    \"\"\"\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result\n",
    "\n",
    "llm_math = LLMMathChain.from_llm(llm=llm, \n",
    "                                 verbose=True) # to see what is going on\n",
    "\n",
    "# call the llm whil \n",
    "count_tokens(llm_math, \"What is the square root of 16?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${{Question with math problem.}}\n",
      "```text\n",
      "${{single line mathematical expression that solves the problem}}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${{Output of running the code}}\n",
      "```\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see inside the prompt\n",
    "print(llm_math.prompt.template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
